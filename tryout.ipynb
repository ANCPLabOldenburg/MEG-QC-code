{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from meg_qc, relative to the path of this file\n",
    "\n",
    "# add meg_qc module to the path\n",
    "\n",
    "from meg_qc.meg_qc_pipeline import make_derivative_meg_qc\n",
    "\n",
    "config_file_path = 'meg_qc/settings.ini'\n",
    "raw, raw_cropped_filtered_resampled, QC_derivs, QC_simple, df_head_pos, head_pos = make_derivative_meg_qc(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "data_file='/Volumes/M2_DATA/MEG_QC_stuff/data/from_Maximilien_Chaumon/ds_emptyroom/sub-Paris/ses-01/sub-Paris_ses-01_task-EmptyRoom_acq-crosstalk_meg.fif'\n",
    "raw = mne.io.read_raw_fif(data_file, on_split_missing='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "picks_EOG = mne.pick_types(raw.info, eog=True)\n",
    "eog_ch_name = [raw.info['chs'][name]['ch_name'] for name in picks_EOG]\n",
    "eog_ch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5.1\n",
    "\n",
    "if n % 1 == 0:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.STD_meq_qc import get_noisy_flat_STD_ptp_epochs\n",
    "\n",
    "m_or_g = 'mag'\n",
    "noisy_multipliar = 1\n",
    "flat_multipliar = 1\n",
    "allow_percent_noisy = 70\n",
    "\n",
    "# convert csv file to dataframe:\n",
    "csv_df = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/derivatives/Meg_QC/sub-013/sub-013_ses-1_task-deduction_run-1_desc-std_per_epoch_mag_meg.csv'\n",
    "df_std = pd.read_csv(csv_df, index_col=0)\n",
    "\n",
    "from universal_plots import QC_derivative\n",
    "\n",
    "def get_noisy_flat_STD_ptp_epochs(df_std: pd.DataFrame, ch_type: str, std_or_ptp: str, noisy_multiplier: float, flat_multiplier: float, percent_noisy_flat_allowed: float):\n",
    "    \n",
    "    \"\"\"Compare the std of this channel for this epoch (df_std) TO the mean STD of this particular channel over all time. (or over all epchs!)\n",
    "    Use some multiplier to figure out by how much it is noisier.\"\"\"\n",
    "\n",
    "    epochs = df_std.columns.tolist()\n",
    "    # convert strings to ints:\n",
    "    epochs = [int(ep) for ep in epochs]\n",
    "\n",
    "    df_std['mean'] = df_std.mean(axis=1) #mean of stds for each separate channel over all epochs together\n",
    "\n",
    "    #compare mean std of each channel to std of this channel for every epoch:\n",
    "    df_noisy_epoch=df_std.copy()\n",
    "    df_flat_epoch=df_std.copy()\n",
    "    df_epoch_vs_mean=df_std.copy()\n",
    "\n",
    "    # Now see which channles in epoch are over std_level or under -std_level:\n",
    "    \n",
    "    for ep in epochs:  \n",
    "\n",
    "        df_epoch_vs_mean.iloc[:,ep] = df_epoch_vs_mean.iloc[:,ep]/ df_std.iloc[:, -1]\n",
    "\n",
    "        df_noisy_epoch.iloc[:,ep] = df_noisy_epoch.iloc[:,ep]/ df_std.iloc[:, -1] > noisy_multiplier\n",
    "        df_flat_epoch.iloc[:,ep] = df_flat_epoch.iloc[:,ep]/ df_std.iloc[:, -1] < flat_multiplier\n",
    "\n",
    "        # Now check if the epoch has over 70% of noisy/flat channels in it -> it is a noisy/flat epoch:\n",
    "\n",
    "        df_noisy_epoch.iloc[-1,ep] = df_noisy_epoch.iloc[:,ep].sum() > len(df_noisy_epoch)*percent_noisy_flat_allowed/100\n",
    "        df_flat_epoch.iloc[-1,ep] = df_flat_epoch.iloc[:,ep].sum() > len(df_flat_epoch)*percent_noisy_flat_allowed/100\n",
    "\n",
    "    df_noisy_epoch.rename({df_noisy_epoch.index[-1]: 'noisy: > %s perc' % percent_noisy_flat_allowed}, inplace=True)\n",
    "    df_flat_epoch.rename({df_flat_epoch.index[-1]: 'flat: > %s perc' % percent_noisy_flat_allowed}, inplace=True)\n",
    "\n",
    "    # Create derivatives:\n",
    "    noisy_flat_epochs_derivs = [\n",
    "        QC_derivative(df_epoch_vs_mean, std_or_ptp+'_per_epoch_vs_mean_ratio_'+ch_type, 'df'),\n",
    "        QC_derivative(df_noisy_epoch, 'Noisy_epochs_on_'+std_or_ptp+'_base_'+ch_type, 'df'),\n",
    "        QC_derivative(df_flat_epoch, 'Flat_epochs_on_'+std_or_ptp+'_base_'+ch_type, 'df')]\n",
    "\n",
    "    return noisy_flat_epochs_derivs\n",
    "\n",
    "noisy_flat_epochs_derivs = get_noisy_flat_STD_ptp_epochs(df_std, m_or_g, 'std', noisy_multipliar, flat_multipliar, allow_percent_noisy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "noisy_flat_epochs_derivs[1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meg_qc import get_all_config_params, initial_processing\n",
    "all_qc_params=get_all_config_params('settings.ini')\n",
    "#print('___MEG QC___: ', all_qc_params)\n",
    "\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif' #GOOD ECG CHANNEL\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from lab/mikado/sub_HT05ND16/210811/mikado-1.fif' #NO ECG CHANNEL, GOOD RECONSTRUCT\n",
    "\n",
    "\n",
    "#data_file='/Volumes/M2_DATA/MEG_QC_stuff/data/from lab/forrest_gump_meg/en04ns31_vp15/190524/vp15_block1-1.fif'  #BAD ECG CHANNEL, 2 good eog.\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004229/sub-102/meg/sub-102_task-amnoise_meg.fif' #2EOG channels, both bad\n",
    "\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif'\n",
    "#2EOG channels, both bad. HEAD WORKS\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#normal psd\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind004/ses-01/meg/sub-mind004_ses-01_task-auditory_meg.fif'\n",
    "#normal but difficult psd\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#EOG 061 bad (or rather unusual), EOG 062 good. Mne takes only the good channel and calculates events on base of it -  \n",
    "# my average and other plots are only on base of 1 goodchannel automatically\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-003/ses-01/meg/sub-003_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "\n",
    "#dict_of_dfs_epoch, dict_epochs_mg, channels, raw_bandpass, raw_filtered_resampled, raw_cropped, raw, active_shielding_used = initial_processing(default_settings=all_qc_params['default'], filtering_settings=all_qc_params['Filtering'], epoching_params=all_qc_params['Epoching'], data_file=data_file)\n",
    "\n",
    "raw = mne.io.read_raw_fif(data_file, allow_maxshield=True)\n",
    "raw_cropped = raw.copy()\n",
    "tmin_my_plot=200\n",
    "tmax_my_plot=300\n",
    "duration_my_plot=tmax_my_plot-tmin_my_plot\n",
    "raw_cropped.crop(tmin=tmin_my_plot, tmax=tmax_my_plot)\n",
    "\n",
    "#raw_cropped.drop_channels(ECG_channel_name)\n",
    "\n",
    "raw_cropped\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from source.Head_meg_qc import HEAD_movement_meg_qc\n",
    "\n",
    "head_derivs, simple_metrics_head, head_not_calculated, df_head_pos, head_pos = HEAD_movement_meg_qc(raw_cropped, plot_with_lines=True, plot_annotations=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.preprocessing import annotate_movement, compute_average_dev_head_t\n",
    "from source.universal_plots import QC_derivative\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def make_head_pos_plot(raw, head_pos):\n",
    "\n",
    "    ''' Plot positions and rotations of the head'''\n",
    "\n",
    "    head_derivs = []\n",
    "\n",
    "    original_head_dev_t = mne.transforms.invert_transform(\n",
    "        raw.info['dev_head_t'])\n",
    "    average_head_dev_t = mne.transforms.invert_transform(\n",
    "        compute_average_dev_head_t(raw, head_pos))\n",
    "\n",
    "    #plot using mne:\n",
    "    fig1 = mne.viz.plot_head_positions(head_pos, mode='traces')\n",
    "    #fig1 = mne.viz.plot_head_positions(head_pos_degrees)\n",
    "    for ax, val, val_ori in zip(fig1.axes[::2], average_head_dev_t['trans'][:3, 3],\n",
    "                        original_head_dev_t['trans'][:3, 3]):\n",
    "        ax.axhline(1000*val, color='r')\n",
    "        ax.axhline(1000*val_ori, color='g')\n",
    "        #print('___MEG QC___: ', 'val', val, 'val_ori', val_ori)\n",
    "    # The green horizontal lines represent the original head position, whereas the\n",
    "    # Red lines are the new head position averaged over all the time points.\n",
    "\n",
    "    head_derivs += [QC_derivative(fig1, 'Head_position_rotation_average', 'matplotlib', description_for_user = 'The green horizontal lines - original head position. Red lines - the new head position averaged over all the time points.')]\n",
    "\n",
    "\n",
    "    #plot head_pos using plotly\n",
    "\n",
    "    # First, for each head position subtract the first point from all the other points to make it always deviate from 0:\n",
    "    head_pos_baselined=head_pos.copy()\n",
    "    #head_pos_baselined=head_pos_degrees.copy()\n",
    "    for i, pos in enumerate(head_pos_baselined.T[1:7]):\n",
    "        pos -= pos[0]\n",
    "        head_pos_baselined.T[i]=pos\n",
    "\n",
    "    t = head_pos.T[0]\n",
    "\n",
    "    average_head_pos=average_head_dev_t['trans'][:3, 3]\n",
    "    original_head_pos=original_head_dev_t['trans'][:3, 3]\n",
    "\n",
    "    fig1p = make_subplots(rows=3, cols=2, subplot_titles=(\"Position (mm)\", \"Rotation (quats)\"))\n",
    "\n",
    "    # head_pos ndarray of shape (n_pos, 10): [t, q1, q2, q3, x, y, z, gof, err, v]\n",
    "    # https://mne.tools/stable/generated/mne.chpi.compute_head_pos.html\n",
    "    indexes=[4, 5, 6, 1, 2,3]\n",
    "    names=['x', 'y', 'z', 'q1', 'q2', 'q3']\n",
    "    for counter in [0, 1, 2]:\n",
    "        position=1000*-head_pos.T[indexes][counter]\n",
    "        name_pos=names[counter]\n",
    "        fig1p.add_trace(go.Scatter(x=t, y=position, mode='lines', name=name_pos), row=counter+1, col=1)\n",
    "        fig1p.update_yaxes(title_text=name_pos, row=counter+1, col=1)\n",
    "        #print('name', name_pos, 'position', position)\n",
    "        rotation=1000*head_pos.T[indexes][counter+3]\n",
    "        name_rot=names[counter+3]\n",
    "        fig1p.add_trace(go.Scatter(x=t, y=rotation, mode='lines', name=name_rot), row=counter+1, col=2)\n",
    "        fig1p.update_yaxes(title_text=name_rot, row=counter+1, col=2)\n",
    "        #print('name', name_rot, 'rotation', rotation)\n",
    "\n",
    "        # fig1p.add_hline(y=1000*average_head_pos[counter], line_dash=\"dash\", line_color=\"red\", row=counter+1, col=1)\n",
    "        # fig1p.add_hline(y=1000*original_head_pos[counter], line_dash=\"dash\", line_color=\"green\", row=counter+1, col=1)\n",
    "\n",
    "    fig1p.update_xaxes(title_text='Time (s)', row=3, col=1)\n",
    "    fig1p.update_xaxes(title_text='Time (s)', row=3, col=2)\n",
    "    fig1p.show()\n",
    "    head_derivs += [QC_derivative(fig1p, 'Head_position_rotation_average_plotly', 'plotly', description_for_user = 'The green horizontal lines - original head position. Red lines - the new head position averaged over all the time points.')]\n",
    "\n",
    "    return head_derivs, head_pos_baselined\n",
    "\n",
    "head_derivs, head_pos_baselined= make_head_pos_plot(raw_cropped, head_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.universal_plots import get_tit_and_unit\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from source.PSD_meg_qc import make_simple_metric_psd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_blended_freqs_old(noisy_freq_bands_idx, width_heights, freqs):\n",
    "\n",
    "    band = 0\n",
    "    while band < len(noisy_freq_bands_idx):\n",
    "\n",
    "        # Checking if the last element of every band is contained in the current band\n",
    "        last = 0\n",
    "        while last < len(noisy_freq_bands_idx):\n",
    "\n",
    "            if (noisy_freq_bands_idx[last] != noisy_freq_bands_idx[band]) and (noisy_freq_bands_idx[last][-1] in noisy_freq_bands_idx[band]):\n",
    "                \n",
    "                #if yes - split the biggest band at the split point and also assign the same heights of peaks to both parts.\n",
    "\n",
    "                split_index = noisy_freq_bands_idx[band].index(noisy_freq_bands_idx[last][-1])\n",
    "                #split_index = noisy_freq_bands_idx[last][-1] ???\n",
    "\n",
    "                split_band_left = noisy_freq_bands_idx[band][:split_index+1]\n",
    "                split_band_right = noisy_freq_bands_idx[band][split_index+1:]\n",
    "\n",
    "\n",
    "                noisy_freq_bands_idx[last] = split_band_left\n",
    "                noisy_freq_bands_idx[band] = split_band_right\n",
    "\n",
    "                min_width_heights = min(width_heights[last],width_heights[band])\n",
    "                width_heights[band] = min_width_heights\n",
    "                width_heights[last] = min_width_heights\n",
    "\n",
    "\n",
    "                #set both bands to 0, so next time  the check will be done for all the bands from the beginning, \n",
    "                # concedering new state of noisy_freq_bands_idx:\n",
    "                band = 0\n",
    "                last = 0\n",
    "\n",
    "            last += 1\n",
    "        band += 1\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "def split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs):\n",
    "\n",
    "    print('___MEG QC___: ', 'peaks_neg', peaks_neg)\n",
    "    print('___MEG QC___: ', 'width_weights:', width_heights)\n",
    "\n",
    "    for n_peak, _ in enumerate(peaks):\n",
    "\n",
    "        #find negative peaks before and after closest to the found positive noise peak.\n",
    "  \n",
    "        neg_peak_before=peaks_neg[np.argwhere(peaks_neg<peaks[n_peak])[-1][0]]\n",
    "        neg_peak_after=peaks_neg[np.argwhere(peaks_neg>peaks[n_peak])[0][0]]\n",
    "\n",
    "        #print('___MEG QC___: ', 'target peak', peaks[n_peak])\n",
    "        #print('___MEG QC___: ', 'before and after', neg_peak_before, neg_peak_after)\n",
    "     \n",
    "        if noisy_freq_bands_idx[n_peak][0] < neg_peak_before:\n",
    "            noisy_freq_bands_idx[n_peak] = [i for i in range(neg_peak_before, noisy_freq_bands_idx[n_peak][-1])]\n",
    "            #print('___MEG QC___: ', 'new band', noisy_freq_bands_idx[n_peak])\n",
    "\n",
    "            #if true, then this peak was blended with another one, \n",
    "            # so the bottom of both peaks (this and previous) needs to be brought \n",
    "            # to the same value.\n",
    "            # (except the case when there were no peaks before)\n",
    "            if n_peak>0:\n",
    "                min_width_heights = min(width_heights[n_peak-1],width_heights[[n_peak]])\n",
    "                width_heights[n_peak-1] = min_width_heights\n",
    "                width_heights[n_peak] = min_width_heights\n",
    "\n",
    "        if noisy_freq_bands_idx[n_peak][-1] > neg_peak_after:\n",
    "            noisy_freq_bands_idx[n_peak] = [i for i in range(noisy_freq_bands_idx[n_peak][0], neg_peak_after)]\n",
    "\n",
    "            #if true, then this peak was blended with another one, \n",
    "            # so the bottom of both peaks (this and next) needs to be brought \n",
    "            # to the same value.\n",
    "            # (except the case when there are no peaks after)\n",
    "            if n_peak<len(peaks)-1:\n",
    "                min_width_heights = min(width_heights[n_peak],width_heights[[n_peak+1]])\n",
    "                width_heights[n_peak] = min_width_heights\n",
    "                width_heights[n_peak+1] = min_width_heights\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "\n",
    "def find_number_and_power_of_noise_freqs(freqs, psds, helper_plots: bool, m_or_g):\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Calculate average psd curve over all channels\n",
    "    # 2. Run peak detection on it -> get number of noise freqs\n",
    "    # 2*. Split blended freqs\n",
    "    # 3. Fit curve to the general psd OR cut the noise peaks at the point they start and baseline them to 0.\n",
    "    # 4. Calculate area under the curve for each noisy peak: area is limited to where amplitude crosses the fitted curve. - count from there.\"\"\"\n",
    "\n",
    "    m_or_g_tit, unit = get_tit_and_unit(m_or_g)\n",
    "\n",
    "    #1.\n",
    "    avg_psd=np.mean(psds,axis=0)\n",
    "\n",
    "    #2. \n",
    "     \n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks, _ = find_peaks(avg_psd, prominence=prominence)\n",
    "    peaks_neg, _ = find_peaks(-avg_psd, prominence=prominence)\n",
    "    peaks_neg = np.insert(peaks_neg, 0, 0, axis=0)\n",
    "    peaks_neg = np.append(peaks_neg, len(freqs)-1)\n",
    "    #insert 0 as index of first negative peak and last index as ind of lastr negative peak.\n",
    "\n",
    "\n",
    "    widths, width_heights, left_ips, right_ips = peak_widths(avg_psd, peaks, rel_height=1)\n",
    "\n",
    "    #Plot signal, peaks and contour lines at which the widths where calculated\n",
    "    from PSD_meg_qc import Power_of_band\n",
    "    from universal_plots import plot_pie_chart_freq\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    print('___MEG QC___: ', 'Central Freqs: ', freqs[peaks])\n",
    "    print('___MEG QC___: ', 'Central Amplitudes: ', avg_psd[peaks])\n",
    "    print('___MEG QC___: ', 'width_heights: ', width_heights)\n",
    "\n",
    "\n",
    "    avg_psd_only_signal=avg_psd.copy()\n",
    "    avg_psd_only_peaks=avg_psd.copy()\n",
    "    avg_psd_only_peaks[:]=None\n",
    "    avg_psd_only_peaks_baselined=avg_psd.copy()\n",
    "    avg_psd_only_peaks_baselined[:]=0\n",
    "\n",
    "    noisy_freq_bands_idx=[]\n",
    "    for ip_n, _ in enumerate(peaks):\n",
    "        #noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n]))])\n",
    "\n",
    "        #+1 here because I  will use these values as range,and range in python is usually \"up to the value but not including\", this should fix it to the right rang\n",
    "        noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n])+1)])\n",
    "        if noisy_freq_bands_idx[ip_n][0]==noisy_freq_bands_idx[ip_n-1][-1]:\n",
    "            noisy_freq_bands_idx[ip_n-1].pop(-1)\n",
    "        #in case the las  element of one band is the same as first of another band, remove the last  elemnt of previos.So bands dont cross.\n",
    "\n",
    "    #2*\n",
    "    print('___MEG QC___: ', 'HERE! BEFORE SPLIT')\n",
    "    print('___MEG QC___: ', noisy_freq_bands_idx)\n",
    "    #noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs)\n",
    "\n",
    "    noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs)\n",
    "    print('___MEG QC___: ', 'HERE! AFTER SPLIT')\n",
    "    print('___MEG QC___: ', noisy_freq_bands_idx_split)\n",
    "\n",
    "\n",
    "    #3.\n",
    "    ips_l, ips_r = [], []\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "        ips_l.append(freqs[fr_b][0])\n",
    "        ips_r.append(freqs[fr_b][-1])\n",
    "        \n",
    "        avg_psd_only_signal[fr_b]=None #keep only main psd, remove noise bands, just for visual\n",
    "        avg_psd_only_peaks[fr_b]=avg_psd[fr_b].copy() #keep only noise bands, remove psd, again for visual\n",
    "        avg_psd_only_peaks_baselined[fr_b]=avg_psd[fr_b].copy()-[width_heights_split[fr_n]]*len(avg_psd_only_peaks[fr_b])\n",
    "        #keep only noise bands and baseline them to 0 (remove the signal which is under the noise line)\n",
    "\n",
    "        # clip the values to 0 if they are negative, they might appear in the beginning of psd curve, \n",
    "        # because the first peak might be above even the higher part of psd. should look intp it? \n",
    "        # maybe that pesk should not be seen as peak at all?\n",
    "        avg_psd_only_peaks_baselined=np.array(avg_psd_only_peaks_baselined) \n",
    "        avg_psd_only_peaks_baselined = np.clip(avg_psd_only_peaks_baselined, 0, None) \n",
    "\n",
    "\n",
    "    if helper_plots is True:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(13, 8))\n",
    "\n",
    "        axs[0, 0].plot(freqs,avg_psd)\n",
    "        axs[0, 0].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 0].plot(freqs[peaks_neg], avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 0].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 0].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[0, 1].plot(freqs,avg_psd_only_signal)\n",
    "        axs[0, 1].plot(freqs[peaks], avg_psd_only_signal[peaks], \"x\")\n",
    "        axs[0, 1].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[0, 1].set_title('2. PSD without noise, split blended freqs')\n",
    "        axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 0].plot(freqs,avg_psd_only_peaks)\n",
    "        axs[1, 0].plot(freqs[peaks], avg_psd_only_peaks[peaks], \"x\")\n",
    "        axs[1, 0].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 0].set_title('3. Only noise peaks, split blended freqs')\n",
    "        axs[1, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 1].plot(freqs,avg_psd_only_peaks_baselined)\n",
    "        axs[1, 1].plot(freqs[peaks], avg_psd_only_peaks_baselined[peaks], \"x\")\n",
    "        axs[1, 1].hlines(y=[0]*len(freqs[peaks]), xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 1].set_title('4. Noise peaks brought to basline, split blended freqs')\n",
    "        axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    #4.\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = simps(avg_psd, dx=freq_res) # power of all signal\n",
    "    print('___MEG QC___: ', 'Total power: ', total_power)\n",
    "\n",
    "    all_bp_noise=[]\n",
    "    all_bp_relative=[]\n",
    "    bp_noise_relative_to_signal=[]\n",
    "\n",
    "    avg_psd_only_peaks_baselined_new=np.array([avg_psd_only_peaks_baselined]) \n",
    "\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "\n",
    "        #print('___MEG QC___: ', 'band',  freqs[fr_b][0], freqs[fr_b][-1])\n",
    "        bp_noise, _, bp_relative = Power_of_band(freqs=freqs, f_low = freqs[fr_b][0], f_high= freqs[fr_b][-1], psds=avg_psd_only_peaks_baselined_new)\n",
    "\n",
    "        all_bp_noise+=bp_noise\n",
    "        all_bp_relative+=bp_relative\n",
    "\n",
    "        #Calculate how much of the total power of the average signal goes into each of the noise freqs:\n",
    "        bp_noise_relative_to_signal.append(bp_noise / total_power) # relative power: % of this band in the total bands power for this channel:\n",
    "\n",
    "    bp_noise_relative_to_signal=[r[0] for r in bp_noise_relative_to_signal]\n",
    "\n",
    "    #print('___MEG QC___: ', 'Freq band for each peak:', ips_pair)\n",
    "    print('___MEG QC___: ', 'BP', all_bp_noise)\n",
    "    print('___MEG QC___: ', 'relative BP', all_bp_relative)\n",
    "    print('___MEG QC___: ', 'Amount of noisy freq in total signal', bp_noise_relative_to_signal)\n",
    "\n",
    "\n",
    "    #Legend for the pie chart:\n",
    "    bands_legend=[]\n",
    "    for fr_n, fr in enumerate(freqs[peaks]):\n",
    "        bands_legend.append(str(fr)+' Hz noise: '+str(all_bp_noise[fr_n])+' '+unit)\n",
    "    main_signal_legend='Main signal: '+str(total_power-sum(all_bp_noise))+' '+unit\n",
    "    bands_legend.append(main_signal_legend)\n",
    "    #bands_legend=[str(fr)+' Hz noise' for fr in freqs[peaks]]+['Main signal'] #legend version without showing the abs power\n",
    "\n",
    "    Snr=bp_noise_relative_to_signal+[1-sum(bp_noise_relative_to_signal)]\n",
    "    noise_pie_derivative = plot_pie_chart_freq(mean_relative_freq=Snr, tit='Signal and Noise. '+m_or_g_tit, bands_names=bands_legend)\n",
    "    noise_pie_derivative.content.show()\n",
    "\n",
    "    simple_metric_deriv=make_simple_metric_psd(all_bp_noise, bp_noise_relative_to_signal, m_or_g, freqs, peaks)\n",
    "\n",
    "    return noise_pie_derivative, simple_metric_deriv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_neg=np.array([3,   5,   8,  11,  13,  16,  27, 116])\n",
    "peaks_neg = np.insert(peaks_neg, 0, 0, axis=0)\n",
    "peaks_neg = np.append(peaks_neg, len(freqs))\n",
    "\n",
    "print('___MEG QC___: ', 'new', peaks_neg)\n",
    "\n",
    "peaks=[  4,   7,   9,  12,  14,  21,  33, 119]\n",
    "noisy_freq_bands_idx=[[3, 4], [5, 6, 7], [8, 9, 10], [11, 12], [13, 14, 15, 16], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [116, 117, 118, 119, 120, 121]]\n",
    "width_heights=[2.46641238e-27, 1.91667602e-27, 1.63137469e-27, 1.63569180e-27, 1.70866416e-27, 1.42336105e-27, 2.05153813e-27, 7.53606847e-29]\n",
    "\n",
    "noisy_freq_bands_idx_new, width_heights=split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_or_g='mag'\n",
    "\n",
    "noise_pie_derivative, simple_metric_deriv = find_number_and_power_of_noise_freqs(freqs, psds, True, m_or_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter, argrelextrema\n",
    "import pandas as pd\n",
    "\n",
    "# Distance away from the FBEWMA that data should be removed.\n",
    "DELTA = 88.0\n",
    "# clip data above this value:\n",
    "HIGH_CLIP = 99.0\n",
    "# clip data below this value:\n",
    "LOW_CLIP = -99.0\n",
    "# How many samples to run the FBEWMA over.\n",
    "SPAN = 5\n",
    "\n",
    "def clip_data(np_unclipped, high_clip, low_clip):\n",
    "    ''' Clip unclipped between high_clip and low_clip. \n",
    "    unclipped contains a single column of unclipped data.'''\n",
    "    \n",
    "    # clip data above HIGH_CLIP or below LOW_CLIP\n",
    "    cond_high_clip = (np_unclipped > HIGH_CLIP) | (np_unclipped < LOW_CLIP)\n",
    "    np_clipped = np.where(cond_high_clip, np.nan, np_unclipped)\n",
    "    return np_clipped.tolist()\n",
    "\n",
    "def ewma_fb(df_column, span):\n",
    "    ''' Apply forwards, backwards exponential weighted moving average (EWMA) to df_column. '''\n",
    "    # Forwards EWMA.\n",
    "    df_column_pd = pd.DataFrame(df_column)\n",
    "    fwd = pd.Series.ewm(df_column_pd, span=span).mean()\n",
    "    # Backwards EWMA.\n",
    "    bwd = pd.Series.ewm(df_column_pd[::-1],span=10).mean()\n",
    "    # Add and take the mean of the forwards and backwards EWMA.\n",
    "    stacked_ewma = np.vstack(( fwd, bwd[::-1] ))\n",
    "    fb_ewma = np.mean(stacked_ewma, axis=0)\n",
    "    return fb_ewma\n",
    "\n",
    "def remove_outliers(np_spikey, np_fbewma, delta):\n",
    "    ''' Remove data from df_spikey that is > delta from fbewma. '''\n",
    "    cond_delta = (np.abs(np_spikey-np_fbewma) > delta)\n",
    "    np_remove_outliers = np.where(cond_delta, np.nan, np_spikey)\n",
    "    return pd.DataFrame(np_remove_outliers)\n",
    "\n",
    "\n",
    "y_clipped = clip_data(avg_psd, HIGH_CLIP, LOW_CLIP)\n",
    "y_ewma = ewma_fb(y_clipped, SPAN)\n",
    "y_outliers_removed = remove_outliers(y_clipped, y_ewma, DELTA)\n",
    "y_interpolated = y_outliers_removed.interpolate().to_numpy()\n",
    "\n",
    "# x = freqs, y = avg_psd\n",
    "x = freqs\n",
    "y = avg_psd\n",
    "\n",
    "def filter_freqs_avg_psd(x, y):\n",
    "\n",
    "    # Define window size (width of the convolution window)\n",
    "    window_size = 140\n",
    "\n",
    "    # Define the order of the filter (polynomial order)\n",
    "    filter_order = 2\n",
    "\n",
    "    # apply the filter to the data\n",
    "    return savgol_filter(y, window_size, filter_order)\n",
    "\n",
    "\n",
    "y_filtered = filter_freqs_avg_psd(x, y)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=freqs, y=avg_psd, name='data'))\n",
    "# fig.add_trace(go.Scatter(x=freqs, y=y_filtered, name='new'))\n",
    "fig.add_trace(go.Scatter(x=freqs, y=y_outliers_removed, name='y_outliers_removed'))\n",
    "fig.add_trace(go.Scatter(x=freqs, y=y_interpolated, name='interpolated'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.universal_plots import get_tit_and_unit\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from source.PSD_meg_qc import make_simple_metric_psd\n",
    "import numpy as np\n",
    "\n",
    "def split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs):\n",
    "\n",
    "    band = 0\n",
    "    while band < len(noisy_freq_bands_idx):\n",
    "\n",
    "        # Checking if the last element of every band is contained in the current band\n",
    "        last = 0\n",
    "        while last < len(noisy_freq_bands_idx):\n",
    "\n",
    "            if (noisy_freq_bands_idx[last] != noisy_freq_bands_idx[band]) and (noisy_freq_bands_idx[last][-1] in noisy_freq_bands_idx[band]):\n",
    "                \n",
    "                #if yes - split the biggest band at the split point and also assign the same heights of peaks to both parts.\n",
    "\n",
    "                split_index = noisy_freq_bands_idx[band].index(noisy_freq_bands_idx[last][-1])\n",
    "                #split_index = noisy_freq_bands_idx[last][-1] ???\n",
    "\n",
    "                split_band_left = noisy_freq_bands_idx[band][:split_index+1]\n",
    "                split_band_right = noisy_freq_bands_idx[band][split_index+1:]\n",
    "\n",
    "\n",
    "                noisy_freq_bands_idx[last] = split_band_left\n",
    "                noisy_freq_bands_idx[band] = split_band_right\n",
    "\n",
    "                min_width_heights = min(width_heights[last],width_heights[band])\n",
    "                width_heights[band] = min_width_heights\n",
    "                width_heights[last] = min_width_heights\n",
    "\n",
    "\n",
    "                #set both bands to 0, so next time  the check will be done for all the bands from the beginning, \n",
    "                # concedering new state of noisy_freq_bands_idx:\n",
    "                band = 0\n",
    "                last = 0\n",
    "\n",
    "            last += 1\n",
    "        band += 1\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "\n",
    "def find_number_and_power_of_noise_freqs(freqs, psds, helper_plots: bool, m_or_g):\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Calculate average psd curve over all channels\n",
    "    # 2. Run peak detection on it -> get number of noise freqs\n",
    "    # 2*. Split blended freqs\n",
    "    # 3. Fit curve to the general psd OR cut the noise peaks at the point they start and baseline them to 0.\n",
    "    # 4. Calculate area under the curve for each noisy peak: area is limited to where amplitude crosses the fitted curve. - count from there.\"\"\"\n",
    "\n",
    "    m_or_g_tit, unit = get_tit_and_unit(m_or_g)\n",
    "\n",
    "    #1.\n",
    "    avg_psd=np.mean(psds,axis=0)\n",
    "\n",
    "    #2. \n",
    "     \n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks, _ = find_peaks(avg_psd, prominence=prominence)\n",
    "    \n",
    "    #________\n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks_neg, _ = find_peaks(-avg_psd, prominence=prominence)\n",
    "    peaks_neg=[0]+list(peaks_neg)+[len(avg_psd)-1] #take the first point of psd always as the start of noise-free curve\n",
    "    \n",
    "    peaks_neg_cut=[] \n",
    "    for p_n in range(0, len(peaks_neg)-1):\n",
    "        if avg_psd[peaks_neg][p_n+1]<=avg_psd[peaks_neg][p_n]:\n",
    "            peaks_neg_cut.append(peaks_neg[p_n+1])\n",
    "    peaks_neg_cut=[0]+peaks_neg_cut\n",
    "\n",
    "#_____\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "    # x = freqs, y = avg_pds\n",
    "    x = freqs\n",
    "    y = avg_psd\n",
    "\n",
    "    # Define the logarithmic function\n",
    "    def log_func(x, a, b, c):\n",
    "        return a * np.log(b * x) + c\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(params):\n",
    "        y_fit = log_func(x, *params)\n",
    "        diff = y_fit - y\n",
    "        # Add the constraint of matching the first and last point\n",
    "        first_last_diff = np.sum([(y_fit[0] - y[0]) ** 2, (y_fit[-1] - y[-1]) ** 2])\n",
    "        # return the sum of square of the differences and the constraint\n",
    "        return np.sum(diff ** 2) + first_last_diff\n",
    "\n",
    "    # Initial guesses for the parameters\n",
    "    params_0 = [1, 1, 1]\n",
    "\n",
    "    # Minimize the objective function\n",
    "    result = minimize(objective, params_0)\n",
    "\n",
    "    # Get the best-fit parameters\n",
    "    a, b, c = result.x\n",
    "\n",
    "    # Use the fitted parameters to calculate y values for the logarithmic curve\n",
    "    y_fit_another = log_func(x, a, b, c)\n",
    "\n",
    "#______\n",
    "\n",
    "\n",
    "    #fit polynomial models up to degree 5\n",
    "    fig = go.Figure()\n",
    "    # print('___MEG QC___: ', 'HERE!', freqs[peaks_neg_cut], avg_psd[peaks_neg_cut])\n",
    "    # model1 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 1)\n",
    "    # model2 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 2)\n",
    "    # model3 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 3)\n",
    "    # model4 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 4)\n",
    "    # model5 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 5)\n",
    "\n",
    "    #y = [model1[0]*np.log(freqs)+ model1[1]] \n",
    "\n",
    "    from scipy.optimize import curve_fit\n",
    "\n",
    "    # Define the logarithmic function to fit to the data\n",
    "    def log_func(x, a, b, c):\n",
    "        return a * np.log(b * x) + c\n",
    "\n",
    "    # Use the curve_fit function to fit the logarithmic function to the data\n",
    "    popt_cut, _ = curve_fit(log_func, freqs[peaks_neg_cut], avg_psd[peaks_neg_cut])\n",
    "\n",
    "    # Get the parameters of the fit\n",
    "    a, b, c = popt_cut\n",
    "\n",
    "    popt_uncut, _ = curve_fit(log_func, freqs, avg_psd)\n",
    "\n",
    "    # Get the parameters of the fit\n",
    "    a1, b1, c1 = popt_uncut\n",
    "\n",
    "    # Use the fitted parameters to calculate y values for the logarithmic curve\n",
    "    #y_fit = log_func(x, a, b, c)\n",
    "    y_fit = log_func(freqs, a, b, c)\n",
    "    y_fit_uncut = log_func(freqs, a1, b1, c1)\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=freqs, y=avg_psd, name='data'))\n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=y[0], name='model1')) \n",
    "    fig.add_trace(go.Scatter(x=freqs[peaks_neg_cut], y=avg_psd[peaks_neg_cut], mode='markers',name='dots')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit, name='cut')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit_uncut, name='uncut')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit_another, name='another one')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model1(freqs), name='model1')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model2(freqs), name='model2')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model3(freqs), name='model3')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model4(freqs), name='model4')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model5(freqs), name='model5')) \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    #________\n",
    "\n",
    "\n",
    "    widths, width_heights, left_ips, right_ips = peak_widths(avg_psd, peaks, rel_height=1)\n",
    "\n",
    "    #Plot signal, peaks and contour lines at which the widths where calculated\n",
    "    from PSD_meg_qc import Power_of_band\n",
    "    from universal_plots import plot_pie_chart_freq\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    print('___MEG QC___: ', 'Central Freqs: ', freqs[peaks])\n",
    "    print('___MEG QC___: ', 'Central Amplitudes: ', avg_psd[peaks])\n",
    "    print('___MEG QC___: ', 'width_heights: ', width_heights)\n",
    "\n",
    "\n",
    "    avg_psd_only_signal=avg_psd.copy()\n",
    "    avg_psd_only_peaks=avg_psd.copy()\n",
    "    avg_psd_only_peaks[:]=None\n",
    "    avg_psd_only_peaks_baselined=avg_psd.copy()\n",
    "    avg_psd_only_peaks_baselined[:]=0\n",
    "\n",
    "    noisy_freq_bands_idx=[]\n",
    "    for ip_n, _ in enumerate(peaks):\n",
    "        #noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n]))])\n",
    "\n",
    "        #+1 here because I  will use these values as range,and range in python is usually \"up to the value but not including\", this should fix it to the right rang\n",
    "        noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n])+1)])\n",
    "        if noisy_freq_bands_idx[ip_n][0]==noisy_freq_bands_idx[ip_n-1][-1]:\n",
    "            noisy_freq_bands_idx[ip_n-1].pop(-1)\n",
    "        #in case the las  element of one band is the same as first of another band, remove the last  elemnt of previos.So bands dont cross.\n",
    "\n",
    "    #2*\n",
    "    print('___MEG QC___: ', 'HERE! BEFORE SPLIT')\n",
    "    print('___MEG QC___: ', noisy_freq_bands_idx)\n",
    "    noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs)\n",
    "    print('___MEG QC___: ', 'HERE! AFTER SPLIT')\n",
    "    print('___MEG QC___: ', noisy_freq_bands_idx_split)\n",
    "\n",
    "\n",
    "    #3.\n",
    "    ips_l, ips_r = [], []\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "        ips_l.append(freqs[fr_b][0])\n",
    "        ips_r.append(freqs[fr_b][-1])\n",
    "        \n",
    "        avg_psd_only_signal[fr_b]=None #keep only main psd, remove noise bands, just for visual\n",
    "        avg_psd_only_peaks[fr_b]=avg_psd[fr_b].copy() #keep only noise bands, remove psd, again for visual\n",
    "        avg_psd_only_peaks_baselined[fr_b]=avg_psd[fr_b].copy()-[width_heights_split[fr_n]]*len(avg_psd_only_peaks[fr_b])\n",
    "        #keep only noise bands and baseline them to 0 (remove the signal which is under the noise line)\n",
    "\n",
    "        # clip the values to 0 if they are negative, they might appear in the beginning of psd curve, \n",
    "        # because the first peak might be above even the higher part of psd. should look intp it? \n",
    "        # maybe that pesk should not be seen as peak at all?\n",
    "        avg_psd_only_peaks_baselined=np.array(avg_psd_only_peaks_baselined) \n",
    "        avg_psd_only_peaks_baselined = np.clip(avg_psd_only_peaks_baselined, 0, None) \n",
    "\n",
    "\n",
    "    if helper_plots is True:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(13, 8))\n",
    "\n",
    "        axs[0, 0].plot(freqs,avg_psd)\n",
    "        axs[0, 0].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 0].plot(freqs[peaks_neg], avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 0].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 0].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[0, 1].plot(freqs,avg_psd)\n",
    "        axs[0, 1].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 1].plot(freqs[peaks_neg_cut], avg_psd[peaks_neg_cut], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 1].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 1].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 1].plot(freqs,-avg_psd)\n",
    "        axs[1, 1].plot(freqs[peaks], -avg_psd[peaks], 'x')\n",
    "        axs[1, 1].plot(freqs[peaks_neg], -avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[1, 1].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[1, 1].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        #axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "\n",
    "        # axs[0, 1].plot(freqs,avg_psd_only_signal)\n",
    "        # axs[0, 1].plot(freqs[peaks], avg_psd_only_signal[peaks], \"x\")\n",
    "        # axs[0, 1].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        # axs[0, 1].set_title('2. PSD without noise, split blended freqs')\n",
    "        # axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        # axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 0].plot(freqs,avg_psd_only_peaks)\n",
    "        axs[1, 0].plot(freqs[peaks], avg_psd_only_peaks[peaks], \"x\")\n",
    "        axs[1, 0].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 0].set_title('3. Only noise peaks, split blended freqs')\n",
    "        axs[1, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        # axs[1, 1].plot(freqs,avg_psd_only_peaks_baselined)\n",
    "        # axs[1, 1].plot(freqs[peaks], avg_psd_only_peaks_baselined[peaks], \"x\")\n",
    "        # axs[1, 1].hlines(y=[0]*len(freqs[peaks]), xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        # axs[1, 1].set_title('4. Noise peaks brought to basline, split blended freqs')\n",
    "        # axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        # axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    #4.\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = simps(avg_psd, dx=freq_res) # power of all signal\n",
    "    print('___MEG QC___: ', 'Total power: ', total_power)\n",
    "\n",
    "    all_bp_noise=[]\n",
    "    all_bp_relative=[]\n",
    "    bp_noise_relative_to_signal=[]\n",
    "\n",
    "    avg_psd_only_peaks_baselined_new=np.array([avg_psd_only_peaks_baselined]) \n",
    "\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "\n",
    "        #print('___MEG QC___: ', 'band',  freqs[fr_b][0], freqs[fr_b][-1])\n",
    "        bp_noise, _, bp_relative = Power_of_band(freqs=freqs, f_low = freqs[fr_b][0], f_high= freqs[fr_b][-1], psds=avg_psd_only_peaks_baselined_new)\n",
    "\n",
    "        all_bp_noise+=bp_noise\n",
    "        all_bp_relative+=bp_relative\n",
    "\n",
    "        #Calculate how much of the total power of the average signal goes into each of the noise freqs:\n",
    "        bp_noise_relative_to_signal.append(bp_noise / total_power) # relative power: % of this band in the total bands power for this channel:\n",
    "\n",
    "    bp_noise_relative_to_signal=[r[0] for r in bp_noise_relative_to_signal]\n",
    "\n",
    "    #print('___MEG QC___: ', 'Freq band for each peak:', ips_pair)\n",
    "    print('___MEG QC___: ', 'BP', all_bp_noise)\n",
    "    print('___MEG QC___: ', 'relative BP', all_bp_relative)\n",
    "    print('___MEG QC___: ', 'Amount of noisy freq in total signal', bp_noise_relative_to_signal)\n",
    "\n",
    "\n",
    "    #Legend for the pie chart:\n",
    "    bands_legend=[]\n",
    "    for fr_n, fr in enumerate(freqs[peaks]):\n",
    "        bands_legend.append(str(fr)+' Hz noise: '+str(all_bp_noise[fr_n])+' '+unit)\n",
    "    main_signal_legend='Main signal: '+str(total_power-sum(all_bp_noise))+' '+unit\n",
    "    bands_legend.append(main_signal_legend)\n",
    "    #bands_legend=[str(fr)+' Hz noise' for fr in freqs[peaks]]+['Main signal'] #legend version without showing the abs power\n",
    "\n",
    "    Snr=bp_noise_relative_to_signal+[1-sum(bp_noise_relative_to_signal)]\n",
    "    noise_pie_derivative = plot_pie_chart_freq(mean_relative_freq=Snr, tit='Signal and Noise. '+m_or_g_tit, bands_names=bands_legend)\n",
    "    noise_pie_derivative.content.show()\n",
    "\n",
    "    simple_metric_deriv=make_simple_metric_psd(all_bp_noise, bp_noise_relative_to_signal, m_or_g, freqs, peaks)\n",
    "\n",
    "    return noise_pie_derivative, simple_metric_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PSD_meg_qc import find_number_and_power_of_noise_freqs\n",
    "\n",
    "noise_pie_derivative, simple_metric_deriv = find_number_and_power_of_noise_freqs(freqs, psds, True, m_or_g)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_neg=np.ndarray(3,   5,   8,  11,  13,  16,  27, 116)\n",
    "print('___MEG QC___: ', peaks_neg)\n",
    "np.insert(peaks_neg, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot EOG channel\n",
    "\n",
    "raw.copy().pick_types(meg=False, stim=False,eog=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.ECG_meg_qc import find_affected_channels\n",
    "\n",
    "mag_ch_names = raw.copy().pick_types(meg='mag').ch_names if 'mag' in raw else None\n",
    "grad_ch_names = raw.copy().pick_types(meg='grad').ch_names if 'grad' in raw else None\n",
    "channels = {'mag': mag_ch_names, 'grad': grad_ch_names}\n",
    "sfreq=raw.info['sfreq']\n",
    "m_or_g='mag'\n",
    "tmin=-0.2\n",
    "tmax=0.2\n",
    "#ecg_epochs = mne.preprocessing.create_ecg_epochs(raw, picks=channels[m_or_g], tmin=tmin, tmax=tmax)\n",
    "\n",
    "#ch_name='EOG 061'\n",
    "ch_name='EOG002'\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], ch_name=ch_name, tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_name='EOG003'\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], ch_name=ch_name, tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(ecg_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='ecg', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True, use_abs_of_all_data='False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(ecg_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='ecg', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True, use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-001/ses-01/meg/sub-001_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "raw = mne.io.read_raw_fif(data_file)\n",
    "raw.info['chs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "y=np.array([1,4,5,0,0,0])\n",
    "peaks_l, peaks_m = mne.preprocessing.peak_finder(y)\n",
    "\n",
    "print('___MEG QC___: ', peaks_l)\n",
    "print('___MEG QC___: ', type(peaks_l[0]))\n",
    "print('___MEG QC___: ', len(peaks_l))\n",
    "\n",
    "print('___MEG QC___: ', np.array([87]))\n",
    "print('___MEG QC___: ', type(np.array([87])[0]))\n",
    "#print('___MEG QC___: ', len(np.array(87)))\n",
    "\n",
    "\n",
    "print('___MEG QC___: ', np.ndarray(87))\n",
    "print('___MEG QC___: ', len(np.ndarray(87)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other useful ancp stuff:\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "\n",
    "default_direct = config['DEFAULT']['data_directory']\n",
    "dataset_path = ancpbids.utils.fetch_dataset(default_direct)\n",
    "\n",
    "from ancpbids import BIDSLayout\n",
    "layout = BIDSLayout(dataset_path)\n",
    "\n",
    "list_of_fifs = layout.get(suffix='meg', extension='.fif', return_type='filename')\n",
    "\n",
    "list_of_subs = layout.get_subjects()\n",
    "\n",
    "\n",
    "list_of_entities = layout.get_entities()\n",
    "print('___MEG QC___: ', list_of_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY SEPARATE FUNCS HERE\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif'\n",
    "all_qc_params = get_all_config_params('settings.ini')\n",
    "dict_of_dfs_epoch, dict_epochs_mg, channels, raw_filtered, raw_filtered_resampled, raw_cropped, raw, active_shielding_used = initial_processing(default_settings=all_qc_params['default'], filtering_settings=all_qc_params['Filtering'], epoching_params=all_qc_params['Epoching'], data_file=data_file)\n",
    "\n",
    "m_or_g_chosen = ['mag']\n",
    "sid='009'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#dfs_ptp_amlitude_annot, bad_channels, amplit_annot_with_ch_names = PP_auto_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "#derivs, big_STD_with_value_all_data, small_STD_with_value_all_data = STD_meg_qc(all_qc_params['STD'], channels, dict_epochs_mg, dict_dict_of_dfs_epoch, raw, m_or_g_chosen)\n",
    "\n",
    "derivs = PP_manual_meg_qc(all_qc_params['PTP_manual'], channels, dict_epochs_mg, dict_of_dfs_epoch, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format = PSD_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "derivs = ECG_meg_qc(config, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format = EOG_meg_qc(config, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format, bad_channels = PP_auto_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "print('___MEG QC___: ', \"--- Execution %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "for der in derivs:\n",
    "    if der.content_type == 'plotly':\n",
    "        der.content.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('___MEG QC___: ', figs)\n",
    "all_fig_derivs = figs\n",
    "figures_report = {}\n",
    "for x in range(0, len(all_fig_derivs)):\n",
    "    if all_fig_derivs[x][3]=='plotly':\n",
    "        figures_report[\"f{0}\".format(x)] = plotly.io.to_html(all_fig_derivs[x][0])\n",
    "    elif all_fig_derivs[x][3]=='matplotlib':\n",
    "        figures_report[\"f{0}\".format(x)] = mpld3.fig_to_html(all_fig_derivs[x][0]);\n",
    "\n",
    "print('___MEG QC___: ', figures_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = '''\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>MEG QC: Frequency spectrum Report</title>\n",
    "        <style>body{ margin:0 100;}</style>\n",
    "    </head>\n",
    "    \n",
    "    <body style=\"font-family: Arial\">\n",
    "        <center>\n",
    "        <h1>MEG data quality analysis report</h1>\n",
    "        <br></br>\n",
    "        <!-- *** Section 1 *** --->\n",
    "        <h2>Frequency spectrum per channel</h2>\n",
    "        ''' + html_fig + '''\n",
    "        <p>graph description...</p>\n",
    "        </center>\n",
    "    \n",
    "    </body>\n",
    "</html>'''\n",
    "\n",
    "with open('report_trial.html', 'w', encoding = 'utf8') as f:\n",
    "    f.write(html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read('settings.ini')\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds000117/sub-01/ses-meg/meg/sub-01_ses-meg_task-facerecognition_run-01_meg.fif'\n",
    "# file does not start with a file id tag\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003392/sub-01/meg/sub-01_task-localizer_meg.fif'\n",
    "# SSS frilter. need to allow maxshiled.\n",
    "\n",
    "#data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003694/sub-01/meg/sub-01_task-MEM_run-01_meg.fif'\n",
    "#raw = mne.io.read_raw_fif(data_file, on_split_missing='ignore')\n",
    "\n",
    "\n",
    "#dict_of_dfs_epoch, dict_epochs_mg, channels, raw_bandpass, raw_filtered_resampled, raw_cropped, raw = initial_processing(config, data_file)\n",
    "\n",
    "#data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003922/sub-Mp150285/ses-01/meg/sub-Mp150285_ses-01_acq-crosstalk_meg.fif'\n",
    "#Could not find measurement data: how many in this set, which subjects?\n",
    "\n",
    "\n",
    "data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004229/sub-102/meg/sub-102_task-amnoise_meg.fif'\n",
    "#SSS filter\n",
    "raw = mne.io.read_raw_fif(data_file, allow_maxshield=True)\n",
    "\n",
    "\n",
    "#data_dir ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/not fitting no fif/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "#raw = mne.io.read_raw_ctf(data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne-qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9935021e94fb8139c84e99eee1e38283b77594d3c3ffe9d650161b8f2d967f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
