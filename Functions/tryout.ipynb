{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% TRY TO SAVE DERIVATIVE FROM BIDS DATASET HERE:\n",
    "from main_meg_qc import make_derivative_meg_qc\n",
    "config_file_name = 'settings.ini'\n",
    "raw, QC_derivs, QC_simple, df_head_pos = make_derivative_meg_qc(config_file_name)\n",
    "\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "ch_data=np.array([1,2,3,4,5,6,7,4,3,1,0,-1,-2,-4,-8,-9, -5, -3])\n",
    "#find the index of the max of ch_data:\n",
    "# print(np.argmax(ch_data))\n",
    "# print(np.argmin(ch_data))\n",
    "# print(np.argmax(ch_data)>np.argmin(ch_data))\n",
    "\n",
    "thresh_mean=(max(ch_data) - min(ch_data)) / 5\n",
    "peak_locs, _ = find_peaks(ch_data, prominence=thresh_mean)\n",
    "peak_mags=ch_data[peak_locs]\n",
    "peak_locs_neg, _ = find_peaks(-ch_data, prominence=thresh_mean)\n",
    "peak_mags_neg=ch_data[peak_locs_neg]\n",
    "\n",
    "print(peak_locs, peak_mags, type(peak_locs))\n",
    "print(peak_locs_neg, peak_mags_neg, type(peak_locs_neg))\n",
    "\n",
    "peak_locs=np.concatenate((peak_locs, peak_locs_neg), axis=None)\n",
    "print(peak_locs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Coordinates of the object in the form of (x, y, z)\n",
    "coordinates = np.array([(1, 2, 3), \n",
    "                        (2, 3, 4), \n",
    "                        (3, 4, 5), \n",
    "                        (4, 5, 6)])\n",
    "\n",
    "print('first col', coordinates[1:])\n",
    "# Calculate the distances between each consecutive pair of coordinates\n",
    "distances = np.sqrt(np.sum((coordinates[1:] - coordinates[:-1])**2, axis=1))\n",
    "\n",
    "# Calculate the mean of the distances\n",
    "mean = np.mean(distances)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_dev = np.std(distances)\n",
    "\n",
    "print(\"Standard deviation of movement:\", std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch\n",
    "import configparser\n",
    "import ancpbids\n",
    "import plotly\n",
    "import mpld3\n",
    "\n",
    "from main_meg_qc import make_derivative_meg_qc\n",
    "from initial_meg_qc import get_all_config_params, sanity_check, initial_processing\n",
    "from RMSE_meq_qc import RMSE_meg_qc\n",
    "from PSD_meg_qc import PSD_meg_qc\n",
    "from Peaks_manual_meg_qc import PP_manual_meg_qc\n",
    "from Peaks_auto_meg_qc import PP_auto_meg_qc\n",
    "from ECG_meg_qc import ECG_meg_qc\n",
    "from EOG_meg_qc import EOG_meg_qc\n",
    "from universal_html_report import keep_fig_derivs, make_joined_report\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_meg_qc import get_all_config_params, initial_processing\n",
    "all_qc_params=get_all_config_params('settings.ini')\n",
    "#print(all_qc_params)\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif' #GOOD ECG CHANNEL\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from lab/mikado/sub_HT05ND16/210811/mikado-1.fif' #NO ECG CHANNEL, GOOD RECONSTRUCT\n",
    "\n",
    "\n",
    "#data_file='/Volumes/M2_DATA/MEG_QC_stuff/data/from lab/forrest_gump_meg/en04ns31_vp15/190524/vp15_block1-1.fif'  #BAD ECG CHANNEL, 2 good eog.\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004229/sub-102/meg/sub-102_task-amnoise_meg.fif' #2EOG channels, both bad\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif'\n",
    "#2EOG channels, both bad\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#normal psd\n",
    "\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind004/ses-01/meg/sub-mind004_ses-01_task-auditory_meg.fif'\n",
    "#normal but difficult psd\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#EOG 061 bad (or rather unusual), EOG 062 good. Mne takes only the good channel and calculates events on base of it -  \n",
    "# my average and other plots are only on base of 1 goodchannel automatically\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-003/ses-01/meg/sub-003_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "\n",
    "#dict_of_dfs_epoch, dict_epochs_mg, channels, raw_bandpass, raw_filtered_resampled, raw_cropped, raw, active_shielding_used = initial_processing(default_settings=all_qc_params['default'], filtering_settings=all_qc_params['Filtering'], epoching_params=all_qc_params['Epoching'], data_file=data_file)\n",
    "\n",
    "raw2 = mne.io.read_raw_fif(data_file, allow_maxshield=True)\n",
    "raw_cropped = raw2.copy()\n",
    "tmin_my_plot=200\n",
    "tmax_my_plot=300\n",
    "duration_my_plot=tmax_my_plot-tmin_my_plot\n",
    "raw_cropped.crop(tmin=tmin_my_plot, tmax=tmax_my_plot)\n",
    "\n",
    "#raw_cropped.drop_channels(ECG_channel_name)\n",
    "\n",
    "raw_cropped\n",
    "raw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use it filtered 0.5-100\n",
    "\n",
    "raw2.crop(tmin=0, tmax=120)\n",
    "raw2.load_data(verbose=True) #Data has to be loaded into mememory before filtering:\n",
    "raw2_filtered = raw2.copy()\n",
    "l_freq=0.5\n",
    "h_freq=100\n",
    "\n",
    "raw2_filtered.filter(l_freq=l_freq, h_freq=h_freq, picks='meg', method='iir', iir_params=None)\n",
    "m_or_g='mag'\n",
    "\n",
    "psds, freqs = raw2_filtered.compute_psd(method='welch', fmin=l_freq, fmax=h_freq, picks=m_or_g, n_jobs=-1, n_fft=2500, n_per_seg=2500).get_data(return_freqs=True)\n",
    "\n",
    "raw2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks, peak_widths\n",
    "\n",
    "avg_psd=np.mean(psds,axis=0)\n",
    "freqs\n",
    "\n",
    "prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "peaks, _ = find_peaks(avg_psd, prominence=prominence)\n",
    "\n",
    "noisy_freqs=freqs[peaks]\n",
    "\n",
    "print(noisy_freqs[:])\n",
    "\n",
    "raw2_filtered_notched=raw2_filtered.copy()\n",
    "\n",
    "for fr in noisy_freqs:\n",
    "    raw2_filtered_notched.notch_filter(freqs=fr,picks='meg', method='iir')\n",
    "\n",
    "psds2, freqs2 = raw2_filtered_notched.compute_psd(method='welch', fmin=l_freq, fmax=h_freq, picks=m_or_g, n_jobs=-1, n_fft=2500, n_per_seg=2500).get_data(return_freqs=True)\n",
    "\n",
    "avg_psd2=np.mean(psds2,axis=0)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=freqs, y=avg_psd, name='data'))\n",
    "fig.add_trace(go.Scatter(x=freqs, y=avg_psd2, name='data notched'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_psd)\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universal_plots import get_tit_and_unit\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from PSD_meg_qc import make_simple_metric_psd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_blended_freqs_old(noisy_freq_bands_idx, width_heights, freqs):\n",
    "\n",
    "    band = 0\n",
    "    while band < len(noisy_freq_bands_idx):\n",
    "\n",
    "        # Checking if the last element of every band is contained in the current band\n",
    "        last = 0\n",
    "        while last < len(noisy_freq_bands_idx):\n",
    "\n",
    "            if (noisy_freq_bands_idx[last] != noisy_freq_bands_idx[band]) and (noisy_freq_bands_idx[last][-1] in noisy_freq_bands_idx[band]):\n",
    "                \n",
    "                #if yes - split the biggest band at the split point and also assign the same heights of peaks to both parts.\n",
    "\n",
    "                split_index = noisy_freq_bands_idx[band].index(noisy_freq_bands_idx[last][-1])\n",
    "                #split_index = noisy_freq_bands_idx[last][-1] ???\n",
    "\n",
    "                split_band_left = noisy_freq_bands_idx[band][:split_index+1]\n",
    "                split_band_right = noisy_freq_bands_idx[band][split_index+1:]\n",
    "\n",
    "\n",
    "                noisy_freq_bands_idx[last] = split_band_left\n",
    "                noisy_freq_bands_idx[band] = split_band_right\n",
    "\n",
    "                min_width_heights = min(width_heights[last],width_heights[band])\n",
    "                width_heights[band] = min_width_heights\n",
    "                width_heights[last] = min_width_heights\n",
    "\n",
    "\n",
    "                #set both bands to 0, so next time  the check will be done for all the bands from the beginning, \n",
    "                # concedering new state of noisy_freq_bands_idx:\n",
    "                band = 0\n",
    "                last = 0\n",
    "\n",
    "            last += 1\n",
    "        band += 1\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "def split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs):\n",
    "\n",
    "    print('peaks_neg', peaks_neg)\n",
    "    print('width_weights:', width_heights)\n",
    "\n",
    "    for n_peak, _ in enumerate(peaks):\n",
    "\n",
    "        #find negative peaks before and after closest to the found positive noise peak.\n",
    "  \n",
    "        neg_peak_before=peaks_neg[np.argwhere(peaks_neg<peaks[n_peak])[-1][0]]\n",
    "        neg_peak_after=peaks_neg[np.argwhere(peaks_neg>peaks[n_peak])[0][0]]\n",
    "\n",
    "        #print('target peak', peaks[n_peak])\n",
    "        #print('before and after', neg_peak_before, neg_peak_after)\n",
    "     \n",
    "        if noisy_freq_bands_idx[n_peak][0] < neg_peak_before:\n",
    "            noisy_freq_bands_idx[n_peak] = [i for i in range(neg_peak_before, noisy_freq_bands_idx[n_peak][-1])]\n",
    "            #print('new band', noisy_freq_bands_idx[n_peak])\n",
    "\n",
    "            #if true, then this peak was blended with another one, \n",
    "            # so the bottom of both peaks (this and previous) needs to be brought \n",
    "            # to the same value.\n",
    "            # (except the case when there were no peaks before)\n",
    "            if n_peak>0:\n",
    "                min_width_heights = min(width_heights[n_peak-1],width_heights[[n_peak]])\n",
    "                width_heights[n_peak-1] = min_width_heights\n",
    "                width_heights[n_peak] = min_width_heights\n",
    "\n",
    "        if noisy_freq_bands_idx[n_peak][-1] > neg_peak_after:\n",
    "            noisy_freq_bands_idx[n_peak] = [i for i in range(noisy_freq_bands_idx[n_peak][0], neg_peak_after)]\n",
    "\n",
    "            #if true, then this peak was blended with another one, \n",
    "            # so the bottom of both peaks (this and next) needs to be brought \n",
    "            # to the same value.\n",
    "            # (except the case when there are no peaks after)\n",
    "            if n_peak<len(peaks)-1:\n",
    "                min_width_heights = min(width_heights[n_peak],width_heights[[n_peak+1]])\n",
    "                width_heights[n_peak] = min_width_heights\n",
    "                width_heights[n_peak+1] = min_width_heights\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "\n",
    "def find_number_and_power_of_noise_freqs(freqs, psds, helper_plots: bool, m_or_g):\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Calculate average psd curve over all channels\n",
    "    # 2. Run peak detection on it -> get number of noise freqs\n",
    "    # 2*. Split blended freqs\n",
    "    # 3. Fit curve to the general psd OR cut the noise peaks at the point they start and baseline them to 0.\n",
    "    # 4. Calculate area under the curve for each noisy peak: area is limited to where amplitude crosses the fitted curve. - count from there.\"\"\"\n",
    "\n",
    "    m_or_g_tit, unit = get_tit_and_unit(m_or_g)\n",
    "\n",
    "    #1.\n",
    "    avg_psd=np.mean(psds,axis=0)\n",
    "\n",
    "    #2. \n",
    "     \n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks, _ = find_peaks(avg_psd, prominence=prominence)\n",
    "    peaks_neg, _ = find_peaks(-avg_psd, prominence=prominence)\n",
    "    peaks_neg = np.insert(peaks_neg, 0, 0, axis=0)\n",
    "    peaks_neg = np.append(peaks_neg, len(freqs)-1)\n",
    "    #insert 0 as index of first negative peak and last index as ind of lastr negative peak.\n",
    "\n",
    "\n",
    "    widths, width_heights, left_ips, right_ips = peak_widths(avg_psd, peaks, rel_height=1)\n",
    "\n",
    "    #Plot signal, peaks and contour lines at which the widths where calculated\n",
    "    from PSD_meg_qc import Power_of_band\n",
    "    from universal_plots import plot_pie_chart_freq\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    print('Central Freqs: ', freqs[peaks])\n",
    "    print('Central Amplitudes: ', avg_psd[peaks])\n",
    "    print('width_heights: ', width_heights)\n",
    "\n",
    "\n",
    "    avg_psd_only_signal=avg_psd.copy()\n",
    "    avg_psd_only_peaks=avg_psd.copy()\n",
    "    avg_psd_only_peaks[:]=None\n",
    "    avg_psd_only_peaks_baselined=avg_psd.copy()\n",
    "    avg_psd_only_peaks_baselined[:]=0\n",
    "\n",
    "    noisy_freq_bands_idx=[]\n",
    "    for ip_n, _ in enumerate(peaks):\n",
    "        #noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n]))])\n",
    "\n",
    "        #+1 here because I  will use these values as range,and range in python is usually \"up to the value but not including\", this should fix it to the right rang\n",
    "        noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n])+1)])\n",
    "        if noisy_freq_bands_idx[ip_n][0]==noisy_freq_bands_idx[ip_n-1][-1]:\n",
    "            noisy_freq_bands_idx[ip_n-1].pop(-1)\n",
    "        #in case the las  element of one band is the same as first of another band, remove the last  elemnt of previos.So bands dont cross.\n",
    "\n",
    "    #2*\n",
    "    print('HERE! BEFORE SPLIT')\n",
    "    print(noisy_freq_bands_idx)\n",
    "    #noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs)\n",
    "\n",
    "    noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs)\n",
    "    print('HERE! AFTER SPLIT')\n",
    "    print(noisy_freq_bands_idx_split)\n",
    "\n",
    "\n",
    "    #3.\n",
    "    ips_l, ips_r = [], []\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "        ips_l.append(freqs[fr_b][0])\n",
    "        ips_r.append(freqs[fr_b][-1])\n",
    "        \n",
    "        avg_psd_only_signal[fr_b]=None #keep only main psd, remove noise bands, just for visual\n",
    "        avg_psd_only_peaks[fr_b]=avg_psd[fr_b].copy() #keep only noise bands, remove psd, again for visual\n",
    "        avg_psd_only_peaks_baselined[fr_b]=avg_psd[fr_b].copy()-[width_heights_split[fr_n]]*len(avg_psd_only_peaks[fr_b])\n",
    "        #keep only noise bands and baseline them to 0 (remove the signal which is under the noise line)\n",
    "\n",
    "        # clip the values to 0 if they are negative, they might appear in the beginning of psd curve, \n",
    "        # because the first peak might be above even the higher part of psd. should look intp it? \n",
    "        # maybe that pesk should not be seen as peak at all?\n",
    "        avg_psd_only_peaks_baselined=np.array(avg_psd_only_peaks_baselined) \n",
    "        avg_psd_only_peaks_baselined = np.clip(avg_psd_only_peaks_baselined, 0, None) \n",
    "\n",
    "\n",
    "    if helper_plots is True:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(13, 8))\n",
    "\n",
    "        axs[0, 0].plot(freqs,avg_psd)\n",
    "        axs[0, 0].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 0].plot(freqs[peaks_neg], avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 0].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 0].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[0, 1].plot(freqs,avg_psd_only_signal)\n",
    "        axs[0, 1].plot(freqs[peaks], avg_psd_only_signal[peaks], \"x\")\n",
    "        axs[0, 1].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[0, 1].set_title('2. PSD without noise, split blended freqs')\n",
    "        axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 0].plot(freqs,avg_psd_only_peaks)\n",
    "        axs[1, 0].plot(freqs[peaks], avg_psd_only_peaks[peaks], \"x\")\n",
    "        axs[1, 0].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 0].set_title('3. Only noise peaks, split blended freqs')\n",
    "        axs[1, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 1].plot(freqs,avg_psd_only_peaks_baselined)\n",
    "        axs[1, 1].plot(freqs[peaks], avg_psd_only_peaks_baselined[peaks], \"x\")\n",
    "        axs[1, 1].hlines(y=[0]*len(freqs[peaks]), xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 1].set_title('4. Noise peaks brought to basline, split blended freqs')\n",
    "        axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    #4.\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = simps(avg_psd, dx=freq_res) # power of all signal\n",
    "    print('Total power: ', total_power)\n",
    "\n",
    "    all_bp_noise=[]\n",
    "    all_bp_relative=[]\n",
    "    bp_noise_relative_to_signal=[]\n",
    "\n",
    "    avg_psd_only_peaks_baselined_new=np.array([avg_psd_only_peaks_baselined]) \n",
    "\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "\n",
    "        #print('band',  freqs[fr_b][0], freqs[fr_b][-1])\n",
    "        bp_noise, _, bp_relative = Power_of_band(freqs=freqs, f_low = freqs[fr_b][0], f_high= freqs[fr_b][-1], psds=avg_psd_only_peaks_baselined_new)\n",
    "\n",
    "        all_bp_noise+=bp_noise\n",
    "        all_bp_relative+=bp_relative\n",
    "\n",
    "        #Calculate how much of the total power of the average signal goes into each of the noise freqs:\n",
    "        bp_noise_relative_to_signal.append(bp_noise / total_power) # relative power: % of this band in the total bands power for this channel:\n",
    "\n",
    "    bp_noise_relative_to_signal=[r[0] for r in bp_noise_relative_to_signal]\n",
    "\n",
    "    #print('Freq band for each peak:', ips_pair)\n",
    "    print('BP', all_bp_noise)\n",
    "    print('relative BP', all_bp_relative)\n",
    "    print('Amount of noisy freq in total signal', bp_noise_relative_to_signal)\n",
    "\n",
    "\n",
    "    #Legend for the pie chart:\n",
    "    bands_legend=[]\n",
    "    for fr_n, fr in enumerate(freqs[peaks]):\n",
    "        bands_legend.append(str(fr)+' Hz noise: '+str(all_bp_noise[fr_n])+' '+unit)\n",
    "    main_signal_legend='Main signal: '+str(total_power-sum(all_bp_noise))+' '+unit\n",
    "    bands_legend.append(main_signal_legend)\n",
    "    #bands_legend=[str(fr)+' Hz noise' for fr in freqs[peaks]]+['Main signal'] #legend version without showing the abs power\n",
    "\n",
    "    Snr=bp_noise_relative_to_signal+[1-sum(bp_noise_relative_to_signal)]\n",
    "    noise_pie_derivative = plot_pie_chart_freq(mean_relative_freq=Snr, tit='Signal and Noise. '+m_or_g_tit, bands_names=bands_legend)\n",
    "    noise_pie_derivative.content.show()\n",
    "\n",
    "    simple_metric_deriv=make_simple_metric_psd(all_bp_noise, bp_noise_relative_to_signal, m_or_g, freqs, peaks)\n",
    "\n",
    "    return noise_pie_derivative, simple_metric_deriv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_neg=np.array([3,   5,   8,  11,  13,  16,  27, 116])\n",
    "peaks_neg = np.insert(peaks_neg, 0, 0, axis=0)\n",
    "peaks_neg = np.append(peaks_neg, len(freqs))\n",
    "\n",
    "print('new', peaks_neg)\n",
    "\n",
    "peaks=[  4,   7,   9,  12,  14,  21,  33, 119]\n",
    "noisy_freq_bands_idx=[[3, 4], [5, 6, 7], [8, 9, 10], [11, 12], [13, 14, 15, 16], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [116, 117, 118, 119, 120, 121]]\n",
    "width_heights=[2.46641238e-27, 1.91667602e-27, 1.63137469e-27, 1.63569180e-27, 1.70866416e-27, 1.42336105e-27, 2.05153813e-27, 7.53606847e-29]\n",
    "\n",
    "noisy_freq_bands_idx_new, width_heights=split_blended_freqs(noisy_freq_bands_idx, peaks, peaks_neg, width_heights, freqs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_or_g='mag'\n",
    "\n",
    "noise_pie_derivative, simple_metric_deriv = find_number_and_power_of_noise_freqs(freqs, psds, True, m_or_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter, argrelextrema\n",
    "import pandas as pd\n",
    "\n",
    "# Distance away from the FBEWMA that data should be removed.\n",
    "DELTA = 88.0\n",
    "# clip data above this value:\n",
    "HIGH_CLIP = 99.0\n",
    "# clip data below this value:\n",
    "LOW_CLIP = -99.0\n",
    "# How many samples to run the FBEWMA over.\n",
    "SPAN = 5\n",
    "\n",
    "def clip_data(np_unclipped, high_clip, low_clip):\n",
    "    ''' Clip unclipped between high_clip and low_clip. \n",
    "    unclipped contains a single column of unclipped data.'''\n",
    "    \n",
    "    # clip data above HIGH_CLIP or below LOW_CLIP\n",
    "    cond_high_clip = (np_unclipped > HIGH_CLIP) | (np_unclipped < LOW_CLIP)\n",
    "    np_clipped = np.where(cond_high_clip, np.nan, np_unclipped)\n",
    "    return np_clipped.tolist()\n",
    "\n",
    "def ewma_fb(df_column, span):\n",
    "    ''' Apply forwards, backwards exponential weighted moving average (EWMA) to df_column. '''\n",
    "    # Forwards EWMA.\n",
    "    df_column_pd = pd.DataFrame(df_column)\n",
    "    fwd = pd.Series.ewm(df_column_pd, span=span).mean()\n",
    "    # Backwards EWMA.\n",
    "    bwd = pd.Series.ewm(df_column_pd[::-1],span=10).mean()\n",
    "    # Add and take the mean of the forwards and backwards EWMA.\n",
    "    stacked_ewma = np.vstack(( fwd, bwd[::-1] ))\n",
    "    fb_ewma = np.mean(stacked_ewma, axis=0)\n",
    "    return fb_ewma\n",
    "\n",
    "def remove_outliers(np_spikey, np_fbewma, delta):\n",
    "    ''' Remove data from df_spikey that is > delta from fbewma. '''\n",
    "    cond_delta = (np.abs(np_spikey-np_fbewma) > delta)\n",
    "    np_remove_outliers = np.where(cond_delta, np.nan, np_spikey)\n",
    "    return pd.DataFrame(np_remove_outliers)\n",
    "\n",
    "\n",
    "y_clipped = clip_data(avg_psd, HIGH_CLIP, LOW_CLIP)\n",
    "y_ewma = ewma_fb(y_clipped, SPAN)\n",
    "y_outliers_removed = remove_outliers(y_clipped, y_ewma, DELTA)\n",
    "y_interpolated = y_outliers_removed.interpolate().to_numpy()\n",
    "\n",
    "# x = freqs, y = avg_psd\n",
    "x = freqs\n",
    "y = avg_psd\n",
    "\n",
    "def filter_freqs_avg_psd(x, y):\n",
    "\n",
    "    # Define window size (width of the convolution window)\n",
    "    window_size = 140\n",
    "\n",
    "    # Define the order of the filter (polynomial order)\n",
    "    filter_order = 2\n",
    "\n",
    "    # apply the filter to the data\n",
    "    return savgol_filter(y, window_size, filter_order)\n",
    "\n",
    "\n",
    "y_filtered = filter_freqs_avg_psd(x, y)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=freqs, y=avg_psd, name='data'))\n",
    "# fig.add_trace(go.Scatter(x=freqs, y=y_filtered, name='new'))\n",
    "fig.add_trace(go.Scatter(x=freqs, y=y_outliers_removed, name='y_outliers_removed'))\n",
    "fig.add_trace(go.Scatter(x=freqs, y=y_interpolated, name='interpolated'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universal_plots import get_tit_and_unit\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from PSD_meg_qc import make_simple_metric_psd\n",
    "import numpy as np\n",
    "\n",
    "def split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs):\n",
    "\n",
    "    band = 0\n",
    "    while band < len(noisy_freq_bands_idx):\n",
    "\n",
    "        # Checking if the last element of every band is contained in the current band\n",
    "        last = 0\n",
    "        while last < len(noisy_freq_bands_idx):\n",
    "\n",
    "            if (noisy_freq_bands_idx[last] != noisy_freq_bands_idx[band]) and (noisy_freq_bands_idx[last][-1] in noisy_freq_bands_idx[band]):\n",
    "                \n",
    "                #if yes - split the biggest band at the split point and also assign the same heights of peaks to both parts.\n",
    "\n",
    "                split_index = noisy_freq_bands_idx[band].index(noisy_freq_bands_idx[last][-1])\n",
    "                #split_index = noisy_freq_bands_idx[last][-1] ???\n",
    "\n",
    "                split_band_left = noisy_freq_bands_idx[band][:split_index+1]\n",
    "                split_band_right = noisy_freq_bands_idx[band][split_index+1:]\n",
    "\n",
    "\n",
    "                noisy_freq_bands_idx[last] = split_band_left\n",
    "                noisy_freq_bands_idx[band] = split_band_right\n",
    "\n",
    "                min_width_heights = min(width_heights[last],width_heights[band])\n",
    "                width_heights[band] = min_width_heights\n",
    "                width_heights[last] = min_width_heights\n",
    "\n",
    "\n",
    "                #set both bands to 0, so next time  the check will be done for all the bands from the beginning, \n",
    "                # concedering new state of noisy_freq_bands_idx:\n",
    "                band = 0\n",
    "                last = 0\n",
    "\n",
    "            last += 1\n",
    "        band += 1\n",
    "\n",
    "    return noisy_freq_bands_idx, width_heights\n",
    "\n",
    "\n",
    "def find_number_and_power_of_noise_freqs(freqs, psds, helper_plots: bool, m_or_g):\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Calculate average psd curve over all channels\n",
    "    # 2. Run peak detection on it -> get number of noise freqs\n",
    "    # 2*. Split blended freqs\n",
    "    # 3. Fit curve to the general psd OR cut the noise peaks at the point they start and baseline them to 0.\n",
    "    # 4. Calculate area under the curve for each noisy peak: area is limited to where amplitude crosses the fitted curve. - count from there.\"\"\"\n",
    "\n",
    "    m_or_g_tit, unit = get_tit_and_unit(m_or_g)\n",
    "\n",
    "    #1.\n",
    "    avg_psd=np.mean(psds,axis=0)\n",
    "\n",
    "    #2. \n",
    "     \n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks, _ = find_peaks(avg_psd, prominence=prominence)\n",
    "    \n",
    "    #________\n",
    "    prominence=(max(avg_psd) - min(avg_psd)) / 50\n",
    "    peaks_neg, _ = find_peaks(-avg_psd, prominence=prominence)\n",
    "    peaks_neg=[0]+list(peaks_neg)+[len(avg_psd)-1] #take the first point of psd always as the start of noise-free curve\n",
    "    \n",
    "    peaks_neg_cut=[] \n",
    "    for p_n in range(0, len(peaks_neg)-1):\n",
    "        if avg_psd[peaks_neg][p_n+1]<=avg_psd[peaks_neg][p_n]:\n",
    "            peaks_neg_cut.append(peaks_neg[p_n+1])\n",
    "    peaks_neg_cut=[0]+peaks_neg_cut\n",
    "\n",
    "#_____\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "    # x = freqs, y = avg_pds\n",
    "    x = freqs\n",
    "    y = avg_psd\n",
    "\n",
    "    # Define the logarithmic function\n",
    "    def log_func(x, a, b, c):\n",
    "        return a * np.log(b * x) + c\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(params):\n",
    "        y_fit = log_func(x, *params)\n",
    "        diff = y_fit - y\n",
    "        # Add the constraint of matching the first and last point\n",
    "        first_last_diff = np.sum([(y_fit[0] - y[0]) ** 2, (y_fit[-1] - y[-1]) ** 2])\n",
    "        # return the sum of square of the differences and the constraint\n",
    "        return np.sum(diff ** 2) + first_last_diff\n",
    "\n",
    "    # Initial guesses for the parameters\n",
    "    params_0 = [1, 1, 1]\n",
    "\n",
    "    # Minimize the objective function\n",
    "    result = minimize(objective, params_0)\n",
    "\n",
    "    # Get the best-fit parameters\n",
    "    a, b, c = result.x\n",
    "\n",
    "    # Use the fitted parameters to calculate y values for the logarithmic curve\n",
    "    y_fit_another = log_func(x, a, b, c)\n",
    "\n",
    "#______\n",
    "\n",
    "\n",
    "    #fit polynomial models up to degree 5\n",
    "    fig = go.Figure()\n",
    "    # print('HERE!', freqs[peaks_neg_cut], avg_psd[peaks_neg_cut])\n",
    "    # model1 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 1)\n",
    "    # model2 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 2)\n",
    "    # model3 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 3)\n",
    "    # model4 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 4)\n",
    "    # model5 = np.polyfit(np.log(freqs[peaks_neg_cut]), avg_psd[peaks_neg_cut], 5)\n",
    "\n",
    "    #y = [model1[0]*np.log(freqs)+ model1[1]] \n",
    "\n",
    "    from scipy.optimize import curve_fit\n",
    "\n",
    "    # Define the logarithmic function to fit to the data\n",
    "    def log_func(x, a, b, c):\n",
    "        return a * np.log(b * x) + c\n",
    "\n",
    "    # Use the curve_fit function to fit the logarithmic function to the data\n",
    "    popt_cut, _ = curve_fit(log_func, freqs[peaks_neg_cut], avg_psd[peaks_neg_cut])\n",
    "\n",
    "    # Get the parameters of the fit\n",
    "    a, b, c = popt_cut\n",
    "\n",
    "    popt_uncut, _ = curve_fit(log_func, freqs, avg_psd)\n",
    "\n",
    "    # Get the parameters of the fit\n",
    "    a1, b1, c1 = popt_uncut\n",
    "\n",
    "    # Use the fitted parameters to calculate y values for the logarithmic curve\n",
    "    #y_fit = log_func(x, a, b, c)\n",
    "    y_fit = log_func(freqs, a, b, c)\n",
    "    y_fit_uncut = log_func(freqs, a1, b1, c1)\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=freqs, y=avg_psd, name='data'))\n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=y[0], name='model1')) \n",
    "    fig.add_trace(go.Scatter(x=freqs[peaks_neg_cut], y=avg_psd[peaks_neg_cut], mode='markers',name='dots')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit, name='cut')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit_uncut, name='uncut')) \n",
    "    fig.add_trace(go.Scatter(x=freqs, y=y_fit_another, name='another one')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model1(freqs), name='model1')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model2(freqs), name='model2')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model3(freqs), name='model3')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model4(freqs), name='model4')) \n",
    "    #fig.add_trace(go.Scatter(x=freqs, y=model5(freqs), name='model5')) \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    #________\n",
    "\n",
    "\n",
    "    widths, width_heights, left_ips, right_ips = peak_widths(avg_psd, peaks, rel_height=1)\n",
    "\n",
    "    #Plot signal, peaks and contour lines at which the widths where calculated\n",
    "    from PSD_meg_qc import Power_of_band\n",
    "    from universal_plots import plot_pie_chart_freq\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    print('Central Freqs: ', freqs[peaks])\n",
    "    print('Central Amplitudes: ', avg_psd[peaks])\n",
    "    print('width_heights: ', width_heights)\n",
    "\n",
    "\n",
    "    avg_psd_only_signal=avg_psd.copy()\n",
    "    avg_psd_only_peaks=avg_psd.copy()\n",
    "    avg_psd_only_peaks[:]=None\n",
    "    avg_psd_only_peaks_baselined=avg_psd.copy()\n",
    "    avg_psd_only_peaks_baselined[:]=0\n",
    "\n",
    "    noisy_freq_bands_idx=[]\n",
    "    for ip_n, _ in enumerate(peaks):\n",
    "        #noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n]))])\n",
    "\n",
    "        #+1 here because I  will use these values as range,and range in python is usually \"up to the value but not including\", this should fix it to the right rang\n",
    "        noisy_freq_bands_idx.append([fr for fr in np.arange((round(left_ips[ip_n])), round(right_ips[ip_n])+1)])\n",
    "        if noisy_freq_bands_idx[ip_n][0]==noisy_freq_bands_idx[ip_n-1][-1]:\n",
    "            noisy_freq_bands_idx[ip_n-1].pop(-1)\n",
    "        #in case the las  element of one band is the same as first of another band, remove the last  elemnt of previos.So bands dont cross.\n",
    "\n",
    "    #2*\n",
    "    print('HERE! BEFORE SPLIT')\n",
    "    print(noisy_freq_bands_idx)\n",
    "    noisy_freq_bands_idx_split, width_heights_split = split_blended_freqs(noisy_freq_bands_idx, width_heights, freqs)\n",
    "    print('HERE! AFTER SPLIT')\n",
    "    print(noisy_freq_bands_idx_split)\n",
    "\n",
    "\n",
    "    #3.\n",
    "    ips_l, ips_r = [], []\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "        ips_l.append(freqs[fr_b][0])\n",
    "        ips_r.append(freqs[fr_b][-1])\n",
    "        \n",
    "        avg_psd_only_signal[fr_b]=None #keep only main psd, remove noise bands, just for visual\n",
    "        avg_psd_only_peaks[fr_b]=avg_psd[fr_b].copy() #keep only noise bands, remove psd, again for visual\n",
    "        avg_psd_only_peaks_baselined[fr_b]=avg_psd[fr_b].copy()-[width_heights_split[fr_n]]*len(avg_psd_only_peaks[fr_b])\n",
    "        #keep only noise bands and baseline them to 0 (remove the signal which is under the noise line)\n",
    "\n",
    "        # clip the values to 0 if they are negative, they might appear in the beginning of psd curve, \n",
    "        # because the first peak might be above even the higher part of psd. should look intp it? \n",
    "        # maybe that pesk should not be seen as peak at all?\n",
    "        avg_psd_only_peaks_baselined=np.array(avg_psd_only_peaks_baselined) \n",
    "        avg_psd_only_peaks_baselined = np.clip(avg_psd_only_peaks_baselined, 0, None) \n",
    "\n",
    "\n",
    "    if helper_plots is True:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(13, 8))\n",
    "\n",
    "        axs[0, 0].plot(freqs,avg_psd)\n",
    "        axs[0, 0].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 0].plot(freqs[peaks_neg], avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 0].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 0].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[0, 1].plot(freqs,avg_psd)\n",
    "        axs[0, 1].plot(freqs[peaks], avg_psd[peaks], 'x')\n",
    "        axs[0, 1].plot(freqs[peaks_neg_cut], avg_psd[peaks_neg_cut], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[0, 1].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[0, 1].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 1].plot(freqs,-avg_psd)\n",
    "        axs[1, 1].plot(freqs[peaks], -avg_psd[peaks], 'x')\n",
    "        axs[1, 1].plot(freqs[peaks_neg], -avg_psd[peaks_neg], 'o')\n",
    "        xmin_f=[round(l) for l in left_ips]\n",
    "        xmax_f=[round(r) for r in right_ips]\n",
    "        xmin=[freqs[i] for i in xmin_f]\n",
    "        xmax=[freqs[i] for i in xmax_f]\n",
    "        axs[1, 1].hlines(y=width_heights, xmin=xmin, xmax=xmax, color=\"C3\")\n",
    "        axs[1, 1].set_title('1. PSD Welch with peaks, blended freqs not split yet. \\n Shown as detected by peak_widths')\n",
    "        axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        #axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "\n",
    "        # axs[0, 1].plot(freqs,avg_psd_only_signal)\n",
    "        # axs[0, 1].plot(freqs[peaks], avg_psd_only_signal[peaks], \"x\")\n",
    "        # axs[0, 1].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        # axs[0, 1].set_title('2. PSD without noise, split blended freqs')\n",
    "        # axs[0, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        # axs[0, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        axs[1, 0].plot(freqs,avg_psd_only_peaks)\n",
    "        axs[1, 0].plot(freqs[peaks], avg_psd_only_peaks[peaks], \"x\")\n",
    "        axs[1, 0].hlines(y=width_heights, xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        axs[1, 0].set_title('3. Only noise peaks, split blended freqs')\n",
    "        axs[1, 0].set_xlim(freqs[0], freqs[-1])\n",
    "        axs[1, 0].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        # axs[1, 1].plot(freqs,avg_psd_only_peaks_baselined)\n",
    "        # axs[1, 1].plot(freqs[peaks], avg_psd_only_peaks_baselined[peaks], \"x\")\n",
    "        # axs[1, 1].hlines(y=[0]*len(freqs[peaks]), xmin=ips_l, xmax=ips_r, color=\"C3\")\n",
    "        # axs[1, 1].set_title('4. Noise peaks brought to basline, split blended freqs')\n",
    "        # axs[1, 1].set_xlim(freqs[0], freqs[-1])\n",
    "        # axs[1, 1].set_ylim(min(avg_psd)*-1.05, max(avg_psd)*1.05)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    #4.\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    total_power = simps(avg_psd, dx=freq_res) # power of all signal\n",
    "    print('Total power: ', total_power)\n",
    "\n",
    "    all_bp_noise=[]\n",
    "    all_bp_relative=[]\n",
    "    bp_noise_relative_to_signal=[]\n",
    "\n",
    "    avg_psd_only_peaks_baselined_new=np.array([avg_psd_only_peaks_baselined]) \n",
    "\n",
    "    for fr_n, fr_b in enumerate(noisy_freq_bands_idx_split):\n",
    "\n",
    "        #print('band',  freqs[fr_b][0], freqs[fr_b][-1])\n",
    "        bp_noise, _, bp_relative = Power_of_band(freqs=freqs, f_low = freqs[fr_b][0], f_high= freqs[fr_b][-1], psds=avg_psd_only_peaks_baselined_new)\n",
    "\n",
    "        all_bp_noise+=bp_noise\n",
    "        all_bp_relative+=bp_relative\n",
    "\n",
    "        #Calculate how much of the total power of the average signal goes into each of the noise freqs:\n",
    "        bp_noise_relative_to_signal.append(bp_noise / total_power) # relative power: % of this band in the total bands power for this channel:\n",
    "\n",
    "    bp_noise_relative_to_signal=[r[0] for r in bp_noise_relative_to_signal]\n",
    "\n",
    "    #print('Freq band for each peak:', ips_pair)\n",
    "    print('BP', all_bp_noise)\n",
    "    print('relative BP', all_bp_relative)\n",
    "    print('Amount of noisy freq in total signal', bp_noise_relative_to_signal)\n",
    "\n",
    "\n",
    "    #Legend for the pie chart:\n",
    "    bands_legend=[]\n",
    "    for fr_n, fr in enumerate(freqs[peaks]):\n",
    "        bands_legend.append(str(fr)+' Hz noise: '+str(all_bp_noise[fr_n])+' '+unit)\n",
    "    main_signal_legend='Main signal: '+str(total_power-sum(all_bp_noise))+' '+unit\n",
    "    bands_legend.append(main_signal_legend)\n",
    "    #bands_legend=[str(fr)+' Hz noise' for fr in freqs[peaks]]+['Main signal'] #legend version without showing the abs power\n",
    "\n",
    "    Snr=bp_noise_relative_to_signal+[1-sum(bp_noise_relative_to_signal)]\n",
    "    noise_pie_derivative = plot_pie_chart_freq(mean_relative_freq=Snr, tit='Signal and Noise. '+m_or_g_tit, bands_names=bands_legend)\n",
    "    noise_pie_derivative.content.show()\n",
    "\n",
    "    simple_metric_deriv=make_simple_metric_psd(all_bp_noise, bp_noise_relative_to_signal, m_or_g, freqs, peaks)\n",
    "\n",
    "    return noise_pie_derivative, simple_metric_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PSD_meg_qc import find_number_and_power_of_noise_freqs\n",
    "\n",
    "noise_pie_derivative, simple_metric_deriv = find_number_and_power_of_noise_freqs(freqs, psds, True, m_or_g)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_neg=np.ndarray(3,   5,   8,  11,  13,  16,  27, 116)\n",
    "print(peaks_neg)\n",
    "np.insert(peaks_neg, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot EOG channel\n",
    "\n",
    "raw.copy().pick_types(meg=False, stim=False,eog=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ECG_meg_qc import find_affected_channels\n",
    "\n",
    "mag_ch_names = raw.copy().pick_types(meg='mag').ch_names if 'mag' in raw else None\n",
    "grad_ch_names = raw.copy().pick_types(meg='grad').ch_names if 'grad' in raw else None\n",
    "channels = {'mag': mag_ch_names, 'grad': grad_ch_names}\n",
    "sfreq=raw.info['sfreq']\n",
    "m_or_g='mag'\n",
    "tmin=-0.2\n",
    "tmax=0.2\n",
    "#ecg_epochs = mne.preprocessing.create_ecg_epochs(raw, picks=channels[m_or_g], tmin=tmin, tmax=tmax)\n",
    "\n",
    "#ch_name='EOG 061'\n",
    "ch_name='EOG002'\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], ch_name=ch_name, tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_name='EOG003'\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], ch_name=ch_name, tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eog_epochs = mne.preprocessing.create_eog_epochs(raw, picks=channels[m_or_g], tmin=tmin, tmax=tmax)\n",
    "\n",
    "\n",
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(eog_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='EOG', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True,  use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(ecg_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='ecg', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True, use_abs_of_all_data='False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_affected_channels, fig_affected, fig_not_affected, fig_avg=find_affected_channels(ecg_epochs, channels, m_or_g, norm_lvl=1, ecg_or_eog='ecg', thresh_lvl_peakfinder=5, sfreq=sfreq, tmin=tmin, tmax=tmax, plotflag=True, use_abs_of_all_data='flip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-001/ses-01/meg/sub-001_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "raw = mne.io.read_raw_fif(data_file)\n",
    "raw.info['chs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "y=np.array([1,4,5,0,0,0])\n",
    "peaks_l, peaks_m = mne.preprocessing.peak_finder(y)\n",
    "\n",
    "print(peaks_l)\n",
    "print(type(peaks_l[0]))\n",
    "print(len(peaks_l))\n",
    "\n",
    "print(np.array([87]))\n",
    "print(type(np.array([87])[0]))\n",
    "#print(len(np.array(87)))\n",
    "\n",
    "\n",
    "print(np.ndarray(87))\n",
    "print(len(np.ndarray(87)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other useful ancp stuff:\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "\n",
    "default_direct = config['DEFAULT']['data_directory']\n",
    "dataset_path = ancpbids.utils.fetch_dataset(default_direct)\n",
    "\n",
    "from ancpbids import BIDSLayout\n",
    "layout = BIDSLayout(dataset_path)\n",
    "\n",
    "list_of_fifs = layout.get(suffix='meg', extension='.fif', return_type='filename')\n",
    "\n",
    "list_of_subs = layout.get_subjects()\n",
    "\n",
    "\n",
    "list_of_entities = layout.get_entities()\n",
    "print(list_of_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY SEPARATE FUNCS HERE\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif'\n",
    "all_qc_params = get_all_config_params('settings.ini')\n",
    "dict_of_dfs_epoch, dict_epochs_mg, channels, raw_filtered, raw_filtered_resampled, raw_cropped, raw, active_shielding_used = initial_processing(default_settings=all_qc_params['default'], filtering_settings=all_qc_params['Filtering'], epoching_params=all_qc_params['Epoching'], data_file=data_file)\n",
    "\n",
    "m_or_g_chosen = ['mag']\n",
    "sid='009'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#dfs_ptp_amlitude_annot, bad_channels, amplit_annot_with_ch_names = PP_auto_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "#derivs, big_rmse_with_value_all_data, small_rmse_with_value_all_data = RMSE_meg_qc(all_qc_params['RMSE'], channels, dict_epochs_mg, dict_dict_of_dfs_epoch, raw, m_or_g_chosen)\n",
    "\n",
    "derivs = PP_manual_meg_qc(all_qc_params['PTP_manual'], channels, dict_epochs_mg, dict_of_dfs_epoch, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format = PSD_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "derivs = ECG_meg_qc(config, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format = EOG_meg_qc(config, raw, m_or_g_chosen)\n",
    "\n",
    "#out_with_name_and_format, bad_channels = PP_auto_meg_qc(sid, config, channels, raw, m_or_g_chosen)\n",
    "\n",
    "print(\"--- Execution %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "for der in derivs:\n",
    "    if der.content_type == 'plotly':\n",
    "        der.content.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(figs)\n",
    "all_fig_derivs = figs\n",
    "figures_report = {}\n",
    "for x in range(0, len(all_fig_derivs)):\n",
    "    if all_fig_derivs[x][3]=='plotly':\n",
    "        figures_report[\"f{0}\".format(x)] = plotly.io.to_html(all_fig_derivs[x][0])\n",
    "    elif all_fig_derivs[x][3]=='matplotlib':\n",
    "        figures_report[\"f{0}\".format(x)] = mpld3.fig_to_html(all_fig_derivs[x][0]);\n",
    "\n",
    "print(figures_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = '''\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>MEG QC: Frequency spectrum Report</title>\n",
    "        <style>body{ margin:0 100;}</style>\n",
    "    </head>\n",
    "    \n",
    "    <body style=\"font-family: Arial\">\n",
    "        <center>\n",
    "        <h1>MEG data quality analysis report</h1>\n",
    "        <br></br>\n",
    "        <!-- *** Section 1 *** --->\n",
    "        <h2>Frequency spectrum per channel</h2>\n",
    "        ''' + html_fig + '''\n",
    "        <p>graph description...</p>\n",
    "        </center>\n",
    "    \n",
    "    </body>\n",
    "</html>'''\n",
    "\n",
    "with open('report_trial.html', 'w', encoding = 'utf8') as f:\n",
    "    f.write(html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read('settings.ini')\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds000117/sub-01/ses-meg/meg/sub-01_ses-meg_task-facerecognition_run-01_meg.fif'\n",
    "# file does not start with a file id tag\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003392/sub-01/meg/sub-01_task-localizer_meg.fif'\n",
    "# SSS frilter. need to allow maxshiled.\n",
    "\n",
    "#data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003694/sub-01/meg/sub-01_task-MEM_run-01_meg.fif'\n",
    "#raw = mne.io.read_raw_fif(data_file, on_split_missing='ignore')\n",
    "\n",
    "\n",
    "#dict_of_dfs_epoch, dict_epochs_mg, channels, raw_bandpass, raw_filtered_resampled, raw_cropped, raw = initial_processing(config, data_file)\n",
    "\n",
    "#data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003922/sub-Mp150285/ses-01/meg/sub-Mp150285_ses-01_acq-crosstalk_meg.fif'\n",
    "#Could not find measurement data: how many in this set, which subjects?\n",
    "\n",
    "\n",
    "data_file ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004229/sub-102/meg/sub-102_task-amnoise_meg.fif'\n",
    "#SSS filter\n",
    "raw = mne.io.read_raw_fif(data_file, allow_maxshield=True)\n",
    "\n",
    "\n",
    "#data_dir ='/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/not fitting no fif/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "#raw = mne.io.read_raw_ctf(data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mne_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d401ab1bf6dd7bb97662b0feb1321a641d60d04842bfa92b47f7871360972b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
